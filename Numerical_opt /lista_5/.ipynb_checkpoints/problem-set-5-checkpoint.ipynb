{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rafał Nowak\n",
    "# Numerical Optimization\n",
    "\n",
    "## Problem set 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5.1** (5 pts)\n",
    "Complete the implementation of Newton's method (see [Boyd, *Convex Optimization*, $\\S 9.5.2$])\n",
    "<img src=\"Boyd-Newton_method.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def newton( func, initial_x, eps=1e-5, maximum_iterations=65536, linesearch=bisection, *linesearch_args  ):\n",
    "    \"\"\" \n",
    "    Newton's Method\n",
    "    func:               the function to optimize It is called as \"value, gradient, hessian = func( x, 2 )\n",
    "    initial_x:          the starting point\n",
    "    eps:                the maximum allowed error in the resulting stepsize t\n",
    "    maximum_iterations: the maximum allowed number of iterations\n",
    "    linesearch:         the linesearch routine\n",
    "    *linesearch_args:   the extra arguments of linesearch routine\n",
    "    \"\"\"\n",
    "    \n",
    "    if eps <= 0:\n",
    "        raise ValueError(\"Epsilon must be positive\")\n",
    "    x = np.asarray( initial_x.copy() )\n",
    "    \n",
    "    # initialization\n",
    "    values = []\n",
    "    runtimes = []\n",
    "    xs = []\n",
    "    start_time = time.time()\n",
    "    iterations = 0\n",
    "    \n",
    "    # Newton's method updates\n",
    "    while True:\n",
    "        \n",
    "        value, gradient, hessian = func( x , order=2 )\n",
    "        value = np.double( value )\n",
    "        gradient = np.matrix( gradient )\n",
    "        hessian = np.matrix( hessian ) \n",
    "        \n",
    "        # updating the logs\n",
    "        values.append( value )\n",
    "        runtimes.append( time.time() - start_time )\n",
    "        xs.append( x.copy() )\n",
    "\n",
    "        ### TODO: Compute the Newton update direction\n",
    "        direction = _________\n",
    "\n",
    "        ### TODO: Compute the Newton decrement\n",
    "        newton_decrement = ________\n",
    "\n",
    "\n",
    "        if __________________:   ### TODO: TERMINATION CRITERION\n",
    "            break\n",
    "        \n",
    "        t = linesearch(func, x, direction, iterations, *linesearch_args)\n",
    "\n",
    "        ### TODO: update x\n",
    "        x = x + _____________\n",
    "\n",
    "        iterations += 1\n",
    "        if iterations >= maximum_iterations:\n",
    "            raise ValueError(\"Too many iterations\")\n",
    "    \n",
    "    return (x, values, runtimes, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation and compare the results for \n",
    "* function `my_func` from problem 3.3. Remark that you should implement the case `order=2` first.\n",
    "* functions `boyd_example_func` and `quadratic` from problem 3.6.\n",
    "\n",
    "You should use both `exact_line_search` and `backtracking` (from problems 3.3 and 3.4 for `linesearch` parameter).\n",
    "\n",
    "<img width=\"80%\" src=\"Boyd-line_search.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 5.2 (5 pts)**\n",
    "Complete the implementation of Conjugate gradients method (see [Nocedal, Wright, *Numerical Optimization*, $\\S 5.2$])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Nocedal_Wright-CG_FR.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:01:53.079828Z",
     "start_time": "2019-12-19T20:01:53.012386Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "def cg( func, initial_x, eps=1e-5, maximum_iterations=65536, linesearch=bisection, *linesearch_args  ):\n",
    "    \"\"\" \n",
    "    Conjugate Gradient\n",
    "    func:               the function to optimize It is called as \"value, gradient = func( x, 1 )\n",
    "    initial_x:          the starting point\n",
    "    eps:                the maximum allowed error in the resulting stepsize t\n",
    "    maximum_iterations: the maximum allowed number of iterations\n",
    "    linesearch:         the linesearch routine\n",
    "    *linesearch_args:   the extra arguments of linesearch routine\n",
    "    \"\"\"\n",
    "    \n",
    "    if eps <= 0:\n",
    "        raise ValueError(\"Epsilon must be positive\")\n",
    "    x = np.asarray( initial_x.copy() )\n",
    "    \n",
    "    # initialization\n",
    "    values = []\n",
    "    runtimes = []\n",
    "    xs = []\n",
    "    start_time = time.time()\n",
    "    m = len( initial_x )\n",
    "    iterations = 0\n",
    "    direction = np.asmatrix( np.zeros( initial_x.shape ) )\n",
    "    \n",
    "    # conjugate gradient updates\n",
    "    while True:\n",
    "        \n",
    "        value, gradient = func( x , 1 )\n",
    "        value = np.double( value )\n",
    "        gradient = np.asarray( gradient )\n",
    "        \n",
    "        # updating the logs\n",
    "        values.append( value )\n",
    "        runtimes.append( time.time() - start_time )\n",
    "        xs.append( x.copy() )\n",
    "\n",
    "        # if ( TODO: TERMINATION CRITERION ): break\n",
    "        \n",
    "        # beta = TODO: UPDATE BETA\n",
    "        \n",
    "        # reset after #(dimensions) iterations\n",
    "        if iterations % m == 0:\n",
    "            beta = 0\n",
    "        \n",
    "        # direction = TODO: FLETCHER-REEVES CONJUGATE GRADIENT UPDATE\n",
    "        \n",
    "        t = linesearch(func, x, direction, iterations, *linesearch_args)\n",
    "        \n",
    "        x += t * direction\n",
    "\n",
    "        iterations += 1\n",
    "        if iterations >= maximum_iterations:\n",
    "            raise ValueError(\"Too many iterations\")\n",
    "    \n",
    "    return (x, values, runtimes, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the function above but implement the Polak-Riberie formulae in line 47.\n",
    "$$ \\beta_{k+1}^{\\mathtt{PR}} = \\frac{\\nabla f_{k+1}^T(\\nabla f_{k+1} - \\nabla f_k)}{\\|f_k\\|^2}$$\n",
    "\n",
    "Observe that we applied the reset trick in lines 44-45.\n",
    "It is worth reading more implementation hints in section [Nocedal, Wright, *Numerical Optimization*, $\\S 5.2$]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation and compare the efficiency on some test functions\n",
    "* the same as in previous problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5.3** (3 pts)\n",
    "Compare the efficiency (number of function/gradient evaluations) of FR and PR updates in CG method for Powell's optimization problem (PSF):\n",
    "$$ \\min_{-10 \\leq x_i \\leq 10} (x_1+10x_2)^2+5(x_3-x_4)^2+(x_2-2x_3)^4 + 10(x_1-x_4)^4,$$\n",
    "\n",
    "\n",
    "Observe that $f(X^*)=0$ for $X^*=0$.\n",
    "\n",
    "More info about PSF can be found, for example, here http://www.optimization-online.org/DB_FILE/2012/03/3382.pdf.\n",
    "\n",
    "Compare your results with [Nocedal, Wright, *Numerical Optimization*, Table 5.1] (row XPOWELL)\n",
    "<img width=50% src=\"Table51.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5.4 (2 pts)**\n",
    "Show experimentally that affine invariance of Newton's method. \n",
    "\n",
    "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be a convex function.\n",
    "Consider an affine transform $y\\mapsto Ay + b$, where $A \\in \\mathbb{R}^{n\\times n}$ is invertible and\n",
    "$b \\in \\mathbb R^n$.\n",
    "\n",
    "Define the function $g : \\mathbb R^n \\mapsto \\mathbb{R}$ by $g(y) = f(Ay + b)$.\n",
    "Denote by $x^{(k)}$ the k-th iterate of Newton’s method performed on $f$.\n",
    "Denote by $y^{(k)}$ the k-th iterate of Newton’s method performed on $g$.\n",
    "* Show that if $x^{(k)} = Ay^{(k)} + b$, then $x^{(k+1)} = Ay^{(k+1)} + b$.\n",
    "* Show that Newton's decrement does not depend on the coordinates, i.e., show that $λ(x^{(k)}) = λ(y^{(k)} ).$\n",
    "\n",
    "Together, this implies that Newton’s method is affine invariant. As an important consequence,\n",
    "Newton’s method cannot be improved by a change of coordinates, unlike gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5.5 (2 pts)**\n",
    "Show experimentally that conjugate gradient method is *not* affine invariant.\n",
    "\n",
    "\n",
    "For example consider the quadratic (convex) function $f:\\mathbb R^n \\to \\mathbb R$ as follows\n",
    "$$ f(x) = \\frac12 x^T H x - c^T x,$$\n",
    "where $H$ positive semi-definite.\n",
    "\n",
    "Consider an affine transformation $y\\mapsto Ay$, where  $A \\in \\mathbb{R}^{n\\times n}$ is invertible:\n",
    "* Denote by $x^{(0)} , x^{(1)} , x^{(2)}$ the first three iterates of conjugate gradient descent on $f(x)$ initialized at $x^{(0)}$.\n",
    "* Now, let $y^{(0)}$ be the point such that $x^{(0)} = Ay^{(0)}$. Denote by $y^{(0)} , y^{(1)} , y^{(2)}$ the first three iterates of conjugate gradient descent on $g(y) = f(Ay)$ initialized at $y^{(0)}$.\n",
    "* Provide an explicit example of $H, A$ and $x^{(0)}$ such that $x^{(1)} \\neq Ay^{(1)}$ and $x^{(2)} \\neq Ay^{(2)}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
